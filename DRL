import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions import Normal

class DRL_Agent(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=128, lr=1e-3, gamma=0.99, tau=0.005):
        super(DRL_Agent, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_size * 2)  # Mean and std
        )
        self.critic = nn.Sequential(
            nn.Linear(state_size + action_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
        self.target_critic = copy.deepcopy(self.critic)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
        self.gamma = gamma
        self.tau = tau

    def forward(self, state):
        mean, log_std = torch.chunk(self.actor(state), 2, dim=-1)
        std = log_std.exp()
        normal = Normal(mean, std)
        action = normal.rsample()
        log_prob = normal.log_prob(action).sum(dim=-1, keepdim=True)
        return action, log_prob

    def get_q_value(self, state, action):
        return self.critic(torch.cat([state, action], dim=-1))

    def update(self, state, action, reward, next_state, done):
        # Critic update
        next_action, _ = self.forward(next_state)
        target_q = self.target_critic(torch.cat([next_state, next_action], dim=-1))
        target_q = reward + (1 - done) * self.gamma * target_q
        q_value = self.get_q_value(state, action)
        critic_loss = nn.MSELoss()(q_value, target_q)
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor update
        new_action, log_prob = self.forward(state)
        actor_loss = -self.get_q_value(state, new_action).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Target network update
        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

    def select_action(self, state):
        with torch.no_grad():
            mean, _ = torch.chunk(self.actor(state), 2, dim=-1)
            return mean.cpu().numpy()

# Ejemplo de uso (a adaptar a mi núcleo)
if __name__ == "__main__":
    state_size = 10 #Tamaño del estado
    action_size = 2 #Tamaño de la acción
    agent = DRL_Agent(state_size, action_size)
    state = torch.randn(1, state_size)
    action = torch.randn(1, action_size)
    reward = torch.randn(1, 1)
    next_state = torch.randn(1, state_size)
    done = torch.tensor([[0.0]])
    agent.update(state, action, reward, next_state, done)
    selected_action = agent.select_action(state)
    print(selected_action)
